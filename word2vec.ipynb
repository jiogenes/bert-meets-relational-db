{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    '''\n",
    "    u_embeddings : Embedding for center word.\n",
    "    v_embeddings : Embedding for neighbor words.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, emb_size, emb_dimension):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
    "        self.v_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
    "\n",
    "        initrange = 1.0 / self.emb_dimension\n",
    "        torch.nn.init.uniform_(self.u_embeddings.weight.data, -initrange, initrange)\n",
    "        torch.nn.init.constant_(self.v_embeddings.weight.data, 0)\n",
    "\n",
    "    def forward(self, pos_u, pos_v, neg_v):\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        emb_v = self.v_embeddings(pos_v)\n",
    "        emb_neg_v = self.v_embeddings(neg_v)\n",
    "\n",
    "        score = torch.sum(torch.mul(emb_u, emb_v), dim=1)\n",
    "        score = torch.clamp(score, max=10, min=-10)\n",
    "        score = -F.logsigmoid(score)\n",
    "\n",
    "        neg_score = torch.bmm(emb_neg_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        neg_score = torch.clamp(neg_score, max=10, min=-10)\n",
    "        neg_score = -torch.sum(F.logsigmoid(-neg_score), dim=1)\n",
    "\n",
    "        return torch.mean(score + neg_score)\n",
    "\n",
    "    def save_embedding(self, id2word, file_name):\n",
    "        embedding = self.u_embeddings.weight.cpu().data.numpy()\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write('%d %d\\n' % (len(id2word), self.emb_dimension))\n",
    "            for wid, w in id2word.items():\n",
    "                e = ' '.join(map(lambda x: str(x), embedding[wid]))\n",
    "                f.write('%s %s\\n' % (w, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecTrainer:\n",
    "    def __init__(self, input_file, output_file, emb_dimension=100, batch_size=32, window_size=5, iterations=3,\n",
    "                 initial_lr=0.001, min_count=12):\n",
    "\n",
    "        self.data = DataReader(input_file, min_count)\n",
    "        dataset = Word2vecDataset(self.data, window_size)\n",
    "        self.dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                                     shuffle=False, num_workers=0, collate_fn=dataset.collate)\n",
    "\n",
    "        self.output_file_name = output_file\n",
    "        self.emb_size = len(self.data.word2id)\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = iterations\n",
    "        self.initial_lr = initial_lr\n",
    "        self.skip_gram_model = SkipGramModel(self.emb_size, self.emb_dimension)\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
    "        if self.use_cuda:\n",
    "            self.skip_gram_model.cuda()\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        for iteration in range(self.iterations):\n",
    "\n",
    "            print(\"\\n\\n\\nIteration: \" + str(iteration + 1))\n",
    "            optimizer = optim.SparseAdam(self.skip_gram_model.parameters(), lr=self.initial_lr)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(self.dataloader))\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for i, sample_batched in enumerate(tqdm(self.dataloader)):\n",
    "\n",
    "                if len(sample_batched[0]) > 1:\n",
    "                    pos_u = sample_batched[0].to(self.device)\n",
    "                    pos_v = sample_batched[1].to(self.device)\n",
    "                    neg_v = sample_batched[2].to(self.device)\n",
    "\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = self.skip_gram_model.forward(pos_u, pos_v, neg_v)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    running_loss = running_loss * 0.9 + loss.item() * 0.1\n",
    "                    if i > 0 and i % 500 == 0:\n",
    "                        print(\" Loss: \" + str(running_loss))\n",
    "\n",
    "            self.skip_gram_model.save_embedding(self.data.id2word, self.output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 49\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "split_ind = (int)(len(text) * 0.8)\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(text)\n",
    "vocab_size = len(vocab)\n",
    "print('vocab_size:', vocab_size)\n",
    "\n",
    "w2i = {w: i for i, w in enumerate(vocab)}\n",
    "i2w = {i: w for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipgram sample [('about', 'We', 1), ('about', 'are', 1), ('about', 'to', 1), ('about', 'study', 1), ('about', 'We', 0), ('about', 'process.', 0), ('about', 'the', 0), ('about', 'called', 0), ('to', 'are', 1), ('to', 'about', 1)]\n"
     ]
    }
   ],
   "source": [
    "def create_skipgram_dataset(text):\n",
    "    import random\n",
    "    data = []\n",
    "    for i in range(2, len(text) - 2):\n",
    "        data.append((text[i], text[i-2], 1))\n",
    "        data.append((text[i], text[i-1], 1))\n",
    "        data.append((text[i], text[i+1], 1))\n",
    "        data.append((text[i], text[i+2], 1))\n",
    "        # negative sampling\n",
    "        for _ in range(4):\n",
    "            if random.random() < 0.5 or i >= len(text) - 3:\n",
    "                rand_id = random.randint(0, i-1)\n",
    "            else:\n",
    "                rand_id = random.randint(i+3, len(text)-1)\n",
    "            data.append((text[i], text[rand_id], 0))\n",
    "    return data\n",
    "\n",
    "skipgram_train = create_skipgram_dataset(text)\n",
    "print('skipgram sample', skipgram_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.embeddings(focus).view((1, -1))\n",
    "        embed_ctx = self.embeddings(context).view((1, -1))\n",
    "        score = torch.mm(embed_focus, torch.t(embed_ctx))\n",
    "        log_probs = F.logsigmoid(score)\n",
    "    \n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 10]) torch.Size([1, 1, 10])\n",
      "torch.Size([1, 10]) torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4683]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = nn.Embedding(10, 10)\n",
    "focus = embeddings(torch.LongTensor([[0]]))\n",
    "context = embeddings(torch.LongTensor([[1]]))\n",
    "print(focus.size(), context.size())\n",
    "focus = embeddings(torch.LongTensor([[0]])).view((1, -1))\n",
    "context = embeddings(torch.LongTensor([[1]])).view((1, -1))\n",
    "print(focus.size(), context.size())\n",
    "torch.mm(focus, torch.t(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram(\n",
      "  (embeddings): Embedding(49, 100)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embd_size = 100\n",
    "learning_rate = 0.001\n",
    "n_epoch = 100\n",
    "\n",
    "def train_skipgram():\n",
    "    losses = []\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model = SkipGram(vocab_size, embd_size)\n",
    "    print(model)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss = .0\n",
    "        for in_w, out_w, target in skipgram_train:\n",
    "            in_w_var = Variable(torch.LongTensor([w2i[in_w]]))\n",
    "            out_w_var = Variable(torch.LongTensor([w2i[out_w]]))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(in_w_var, out_w_var)\n",
    "            loss = loss_fn(log_probs[0], Variable(torch.Tensor([target])))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        losses.append(total_loss)\n",
    "    return model, losses\n",
    "    \n",
    "sg_model, sg_losses = train_skipgram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2i['directed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Test SkipGram===\n",
      "Accuracy: 50.0% (232/464)\n"
     ]
    }
   ],
   "source": [
    "def test_skipgram(test_data, model):\n",
    "    print('====Test SkipGram===')\n",
    "    correct_ct = 0\n",
    "    for in_w, out_w, target in test_data:\n",
    "        in_w_var = Variable(torch.LongTensor([w2i[in_w]]))\n",
    "        out_w_var = Variable(torch.LongTensor([w2i[out_w]]))\n",
    "\n",
    "        model.zero_grad()\n",
    "        log_probs = model(in_w_var, out_w_var)\n",
    "        _, predicted = torch.max(log_probs.data, 1)\n",
    "        predicted = predicted[0]\n",
    "        if predicted == target:\n",
    "            correct_ct += 1\n",
    "\n",
    "    print('Accuracy: {:.1f}% ({:d}/{:d})'.format(correct_ct/len(test_data)*100, correct_ct, len(test_data)))\n",
    "\n",
    "test_skipgram(skipgram_train, sg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    '''\n",
    "    u_embeddings : Embedding for center word.\n",
    "    v_embeddings : Embedding for neighbor words.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, emb_size, emb_dimension):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
    "        self.v_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
    "\n",
    "        initrange = 1.0 / self.emb_dimension\n",
    "        torch.nn.init.uniform_(self.u_embeddings.weight.data, -initrange, initrange)\n",
    "        torch.nn.init.constant_(self.v_embeddings.weight.data, 0)\n",
    "\n",
    "    def forward(self, pos_u, pos_v, neg_v):\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        emb_v = self.v_embeddings(pos_v)\n",
    "        emb_neg_v = self.v_embeddings(neg_v)\n",
    "\n",
    "        score = torch.sum(torch.mul(emb_u, emb_v), dim=1)\n",
    "        score = torch.clamp(score, max=10, min=-10)\n",
    "        score = -F.logsigmoid(score)\n",
    "\n",
    "        neg_score = torch.bmm(emb_neg_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        neg_score = torch.clamp(neg_score, max=10, min=-10)\n",
    "        neg_score = -torch.sum(F.logsigmoid(-neg_score), dim=1)\n",
    "\n",
    "        return torch.mean(score + neg_score)\n",
    "\n",
    "    def save_embedding(self, id2word, file_name):\n",
    "        embedding = self.u_embeddings.weight.cpu().data.numpy()\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write('%d %d\\n' % (len(id2word), self.emb_dimension))\n",
    "            for wid, w in id2word.items():\n",
    "                e = ' '.join(map(lambda x: str(x), embedding[wid]))\n",
    "                f.write('%s %s\\n' % (w, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipGramModel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('mytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4a27c98caeaa5630be10b8d406a4608184d11e4add7ee29d27ce8c7f4d0bc9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
